import ROOT
import os
from array import array
import pandas as pd
import numpy as np
import json
import glob
import logging as log
import argparse
import pdb

cmsswbase = os.environ['CMSSW_BASE']

parser = argparse.ArgumentParser()
parser.add_argument("-b", "--beginRun", help="first run to analyze [%default]", default=272007)
parser.add_argument("-e", "--endRun", help="analyze stops when comes to this run [%default]", default=1000000)
parser.add_argument('--mcCorrections', default='./Resources/MCCorrections.json', type=str,
                    help='specify .json file with MC corrections for muon correlations')
parser.add_argument("-v", "--verbose", help="increase logging level from INFO to DEBUG", default=False,
                    action="store_true")
parser.add_argument("-c", "--writeSummaryCSV", help="produce merged CSV with all runs", default=True)
parser.add_argument("--dirDQM", help="Directory to the input root files from the DQM Offline module",
                    default="/eos/home-d/dwalter/www/ZCounting/DQM-Offline-2018/")
parser.add_argument("--byLsCSV", help="ByLs csv input generated by testBril.sh",
                    default="/eos/cms/store/group/comm_luminosity/ZCounting/brilcalcFile2018/*csv")
parser.add_argument("--sigTemplates", help="Directory to root file for Z mass template",
                    default=cmsswbase + "/src/ZCounting/Resources/")
parser.add_argument('--ptCut', type=float,
    help='specify lower pt cut on tag and probe muons')
parser.add_argument("-o", "--dirOut", help="where to store the output files", default="./")

args = parser.parse_args()
if args.verbose:
    log.basicConfig(format="%(levelname)s: %(message)s", level=log.DEBUG)
else:
    log.basicConfig(format="%(levelname)s: %(message)s", level=log.INFO)

########## Input configuration ##########
# ByLS csv inputs generated by testBRIL.sh
byLS_filelist = glob.glob(args.byLsCSV)
byLS_filelist.sort(key=os.path.getmtime)
byLS_filename = byLS_filelist[-1]
print("The brilcalc csv file: " + str(byLS_filename))

eosDir = args.dirDQM
mcCorr = args.mcCorrections

# MC inputs: to build MC*Gaussian template for efficiency fitting
mcShapeDir = args.sigTemplates if args.sigTemplates.endswith("/") else args.sigTemplates+"/"

ptCutTag = args.ptCut
ptCutProbe = args.ptCut

outDir = args.dirOut if args.dirOut.endswith("/") else args.dirOut+"/"
if not os.path.isdir(outDir):
    os.mkdir(outDir)


########################################

with open(mcCorr) as json_file:
    corr = json.load(json_file)

########### Constant settings ##########
secPerLS = float(23.3)
currentYear = 2017

#from 2017H low PU, w/o PU corrections in pb. |eta| < 2.4,  pt>30
sigma_fid = 610.1401700042975

maximumLS = 2500
LSperMeasurement = 100  # required number of lumi sections per measurement
LumiPerMeasurement = 20  # minimum recorded lumi for one measurement in pb-1

ZBBRate = 0.077904  # Fraction of Z events where both muons are in barrel region
ZBERate = 0.117200  # barrel and endcap
ZEERate = 0.105541  # both endcap

########################################


# log.info("Loading C marco...")	#I think we don't need this
ROOT.gROOT.LoadMacro(os.path.dirname(os.path.realpath(
    __file__)) + "/calculateDataEfficiency.C")  # load function getZyield(...) and calculateDataEfficiency(...)

# turn off graphical output on screen
ROOT.gROOT.SetBatch(True)

log.info("Loading input byls csv...")
byLS_file = open(str(byLS_filename))
byLS_lines = byLS_file.readlines()
byLS_data = pd.read_csv(byLS_filename, sep=',', low_memory=False,
                   skiprows=lambda x: byLS_lines[x].startswith('#') and not byLS_lines[x].startswith('#run'))
log.debug("%s", byLS_data.axes)
log.info("Loading input byls csv DONE...")
# formatting the csv

byLS_data[['run', 'fill']] = byLS_data['#run:fill'].str.split(':', expand=True).apply(pd.to_numeric)
byLS_data['ls'] = byLS_data['ls'].str.split(':', expand=True)[0].apply(pd.to_numeric)
byLS_data = byLS_data.drop(['#run:fill', 'hltpath', 'source'], axis=1)

if 'delivered(/ub)' in byLS_data.columns.tolist():  # convert to /pb
    byLS_data['delivered(/ub)'] = byLS_data['delivered(/ub)'].apply(lambda x: x / 1000000.)
    byLS_data['recorded(/ub)'] = byLS_data['recorded(/ub)'].apply(lambda x: x / 1000000.)
    byLS_data = byLS_data.rename(index=str, columns={'delivered(/ub)': 'delivered(/pb)', 'recorded(/ub)': 'recorded(/pb)'})
elif 'delivered(/fb)' in byLS_data.columns.tolist():  # convert to /pb
    byLS_data['delivered(/fb)'] = byLS_data['delivered(/fb)'].apply(lambda x: x * 1000.)
    byLS_data['recorded(/fb)'] = byLS_data['recorded(/fb)'].apply(lambda x: x * 1000.)
    byLS_data = byLS_data.rename(index=str, columns={'delivered(/fb)': 'delivered(/pb)', 'recorded(/fb)': 'recorded(/pb)'})

# if there are multiple entries of the same ls (for example from different triggers), only keep the one with the highest luminosity.
byLS_data = byLS_data.sort_values(['fill', 'run', 'ls', 'delivered(/pb)', 'recorded(/pb)'])
byLS_data = byLS_data.drop_duplicates(['fill', 'run', 'ls'])

log.info("Looping over runs...")
for run in byLS_data.drop_duplicates('run')['run'].values:

    data_run = byLS_data.loc[byLS_data['run'] == run]

    fill = data_run.drop_duplicates('fill')['fill'].values[0]
    LSlist = data_run['ls'].values.tolist()

    if run < int(args.beginRun) or run >= int(args.endRun):
        continue

    # check if run was processed already
    outSubDir = outDir + "Run{0}/".format(run)
    if os.path.isdir(outSubDir):
        print("Run " + str(run) + " was already processed, skipping and going to next run")
        continue
    os.mkdir(outSubDir)

    log.info("===Running Run %i", run)
    log.info("===Running Fill %i", fill)

    log.debug("===Setting up arrays for output csv...")
    fillarray = array('i')
    runarray = array('i')
    tdate_begin = array('i')
    tdate_end = array('i')
    Zrate = array('d')
    Zrate_EStatUp = array('d')
    Zrate_EStatDown = array('d')
    ZrateUncorrected = array('d')
    lumiRec = array('d')
    lumiDel = array('d')
    instDel = array('d')
    instRec = array('d')
    pileUp = array('d')
    ZyieldRec = array('d')
    ZyieldDel = array('d')
    windowarray = array('d')
    deadTime = array('d')
    beginLS = array('i')
    endLS = array('i')

    XSecFid = array('d')
    XSecFidUncorrected = array('d')
    Zyield_reco = array('d')
    Zyield_chi2 = array('d')
    Zyield_fpr = array('d')  # Z fals positive rate: fraction of bkg events in reconstructed Zs: bkg/(sig+bkg)
    ZLumiRec = array('d')
    ZLumiDel = array('d')
    ZinstLumiRec = array('d')
    ZinstLumiDel = array('d')

    # Efficiency related
    HLTeffB = array('d')
    HLTeffE = array('d')
    SeleffB = array('d')
    SeleffE = array('d')
    GloeffB = array('d')
    GloeffE = array('d')
    StaeffB = array('d')
    StaeffE = array('d')
    TrkeffB = array('d')
    TrkeffE = array('d')

    HLTeffB_chi2pass = array('d')
    HLTeffB_chi2fail = array('d')
    HLTeffE_chi2pass = array('d')
    HLTeffE_chi2fail = array('d')
    SeleffB_chi2pass = array('d')
    SeleffB_chi2fail = array('d')
    SeleffE_chi2pass = array('d')
    SeleffE_chi2fail = array('d')
    GloeffB_chi2pass = array('d')
    GloeffB_chi2fail = array('d')
    GloeffE_chi2pass = array('d')
    GloeffE_chi2fail = array('d')
    StaeffB_chi2pass = array('d')
    StaeffB_chi2fail = array('d')
    StaeffE_chi2pass = array('d')
    StaeffE_chi2fail = array('d')
    TrkeffB_chi2pass = array('d')
    TrkeffB_chi2fail = array('d')
    TrkeffE_chi2pass = array('d')
    TrkeffE_chi2fail = array('d')

    ZMCeff = array('d')
    ZMCeffBB = array('d')
    ZMCeffBE = array('d')
    ZMCeffEE = array('d')

    Zeff = array('d')
    ZBBeff = array('d')
    ZBEeff = array('d')
    ZEEeff = array('d')

    nMeasurements = 0

    log.info("===Loading input DQMIO.root file...")
    eosFileList = glob.glob(eosDir + '/*/*' + str(run) + '*root')

    if not len(eosFileList) > 0:
        print("The file does not yet exist for run: " + str(run))
        continue
    else:
        eosFile = eosFileList[0]

    print("The file exists: " + str(eosFile) + " for run  " + str(run))
    log.info("===Looping over measurements...")

    with open(outSubDir + 'byLS.csv', 'wb') as file:
        file.write("#run:fill,ls,time,delivered(Hz/pb),recorded(Hz/pb),source\n")

    while len(LSlist) > 0:  # begin next measurement "m"
        log.debug("Openning DQMIO.root file: %s", eosFile)

        mergeMeasurements = False
        # merge data to one measuement if remaining luminosity is too less for two measuements
        if (sum(data_run.loc[data_run['ls'].isin(LSlist)]['recorded(/pb)'].values) < 2 * LumiPerMeasurement):
            mergeMeasurements = True

        # produce goodLSlist with ls that are used for one measurement
        goodLSlist = []
        recLumi_m = 0
        while (recLumi_m < LumiPerMeasurement or len(goodLSlist) < LSperMeasurement or mergeMeasurements) and len(
                LSlist) > 0:
            if len(LSlist) < 1:
                print("No more lumi sections in current run")
                break
            if LSlist[0] > maximumLS:
                log.error("===Lumi Section not stored in root file")
                break

            goodLSlist.append(LSlist[0])
            recLumi_m += (data_run[data_run['ls'] == LSlist[0]]['recorded(/pb)'].values)[0]
            del LSlist[0]

        ### load histograms
        dqmfile = ROOT.TFile(eosFile)

        def load_histo(name_):
            h_X_ls = dqmfile.Get("DQMData/Run {0}/ZCounting/Run summary/Histograms/{1}".format(run, name_))
            h_X = h_X_ls.ProjectionY("h_mass_{0}_{1}".format(name_, run), goodLSlist[0], goodLSlist[0], "e")
            for ls in goodLSlist[1:]:
                h_X.Add(h_X_ls.ProjectionY("h_mass_{0}_{1}_{2}".format(name_, run, ls), ls, ls, "e"))
            return h_X

        ### compute Z yield
        Zyieldres_m = ROOT.getZyield(load_histo("h_mass_yield_Z"), outSubDir, nMeasurements, 0, 0, ptCutTag, ptCutProbe, recLumi_m);

        ### compute muon efficiencies
        HLTeffresB_m = ROOT.calculateDataEfficiency(load_histo("h_mass_HLT_pass_central"), load_histo("h_mass_HLT_fail_central"),
                                                    outSubDir, nMeasurements, "HLT", 0, 2, 5, 2, 5,
                                                    ptCutTag, ptCutProbe, 0, recLumi_m, mcShapeDir + "template_HLT.root")
        HLTeffresE_m = ROOT.calculateDataEfficiency(load_histo("h_mass_HLT_pass_forward"), load_histo("h_mass_HLT_fail_forward"),
                                                    outSubDir, nMeasurements, "HLT", 1, 2, 5, 2, 5,
                                                    ptCutTag, ptCutProbe, 0, recLumi_m, mcShapeDir + "template_HLT.root")

        SeleffresB_m = ROOT.calculateDataEfficiency(load_histo("h_mass_SIT_pass_central"), load_histo("h_mass_SIT_fail_central"),
                                                    outSubDir, nMeasurements, "Sel", 0, 2, 1, 2, 1,
                                                    ptCutTag, ptCutProbe, 0, recLumi_m, mcShapeDir + "template_Sel.root")
        SeleffresE_m = ROOT.calculateDataEfficiency(load_histo("h_mass_SIT_pass_forward"), load_histo("h_mass_SIT_fail_forward"),
                                                    outSubDir, nMeasurements, "Sel", 1, 2, 1, 2, 1,
                                                    ptCutTag, ptCutProbe, 0, recLumi_m, mcShapeDir + "template_Sel.root")

        GloeffresB_m = ROOT.calculateDataEfficiency(load_histo("h_mass_Glo_pass_central"), load_histo("h_mass_Glo_fail_central"),
                                                    outSubDir, nMeasurements, "Glo", 0, 2, 2, 2, 2,
                                                    ptCutTag, ptCutProbe, 0, recLumi_m, mcShapeDir + "template_Glo.root")
        GloeffresE_m = ROOT.calculateDataEfficiency(load_histo("h_mass_Glo_pass_forward"), load_histo("h_mass_Glo_fail_forward"),
                                                    outSubDir, nMeasurements, "Glo", 1, 2, 2, 2, 2,
                                                    ptCutTag, ptCutProbe, 0, recLumi_m, mcShapeDir + "template_Glo.root")

        Zyield_m = Zyieldres_m[0]
        HLTeffB_m = HLTeffresB_m[0]
        HLTeffE_m = HLTeffresE_m[0]
        SeleffB_m = SeleffresB_m[0]
        SeleffE_m = SeleffresE_m[0]
        GloeffB_m = GloeffresB_m[0]
        GloeffE_m = GloeffresE_m[0]

        log.debug("======ZRawYield: %f", Zyield_m)

        # ZtoMuMu efficiency purely from data
        ZBBEff = (GloeffB_m * GloeffB_m * SeleffB_m * SeleffB_m * (1 - (1 - HLTeffB_m) * (1 - HLTeffB_m)))
        ZBEEff = (GloeffB_m * GloeffE_m * SeleffB_m * SeleffE_m * (1 - (1 - HLTeffB_m) * (1 - HLTeffE_m)))
        ZEEEff = (GloeffE_m * GloeffE_m * SeleffE_m * SeleffE_m * (1 - (1 - HLTeffE_m) * (1 - HLTeffE_m)))

        # Statistic Uncertainties (low,high) error propagation
        ZBBEff_EStat = [0., 0.]
        ZBEEff_EStat = [0., 0.]
        ZEEEff_EStat = [0., 0.]
        for i in (1, 2):
            ZBBEff_EStat[i - 1] = 2 * ZBBEff * np.sqrt(
                (GloeffresB_m[i] / GloeffB_m) ** 2 +
                (SeleffresB_m[i] / SeleffB_m) ** 2 +
                ((1 - HLTeffB_m) / (1 - (1 - HLTeffB_m) ** 2) * HLTeffresB_m[i]) ** 2
            )
            ZEEEff_EStat[i - 1] = 2 * ZEEEff * np.sqrt(
                (GloeffresE_m[i] / GloeffE_m) ** 2 +
                (SeleffresE_m[i] / SeleffE_m) ** 2 +
                ((1 - HLTeffE_m) / (1 - (1 - HLTeffE_m) ** 2) * HLTeffresE_m[i]) ** 2
            )
            ZBEEff_EStat[i - 1] = ZBEEff * np.sqrt(
                (GloeffresB_m[i] / GloeffB_m) ** 2 +
                (GloeffresE_m[i] / GloeffE_m) ** 2 +
                (SeleffresB_m[i] / SeleffB_m) ** 2 +
                (SeleffresE_m[i] / SeleffE_m) ** 2 +
                ((1 - HLTeffE_m) / (1 - (1 - HLTeffB_m) * (1 - HLTeffE_m)) * HLTeffresB_m[i]) ** 2 +
                ((1 - HLTeffB_m) / (1 - (1 - HLTeffB_m) * (1 - HLTeffE_m)) * HLTeffresE_m[i]) ** 2
            )

        # ZtoMuMu efficiency correction as a parametrized function of pile-up
        avgpu_m = np.average(data_run.loc[data_run['ls'].isin(goodLSlist)]['avgpu'].values)

        ZMCEffBB = ZBBEff - (corr['BB_a'] * avgpu_m + corr['BB_b'])
        ZMCEffBE = ZBEEff - (corr['BE_a'] * avgpu_m + corr['BE_b'])
        ZMCEffEE = ZEEEff - (corr['EE_a'] * avgpu_m + corr['EE_b'])


        # Multiply average frequency of each category with its efficiency
        ZMCEff = (ZMCEffBB * ZBBRate + ZMCEffBE * ZBERate + ZMCEffEE * ZEERate) / (ZBBRate + ZBERate + ZEERate)
        ZEff = (ZBBEff * ZBBRate + ZBEEff * ZBERate + ZEEEff * ZEERate) / (ZBBRate + ZBERate + ZEERate)
        ZEff_EStat = [0., 0.]
        for i in (0, 1):
            ZEff_EStat[i] = 1. / (ZBBRate + ZBERate + ZEERate) * np.sqrt(
                (ZBBRate * ZBBEff_EStat[i]) ** 2 + (ZBERate * ZBEEff_EStat[i]) ** 2 + (ZEERate * ZEEEff_EStat[i]) ** 2
            )

        log.debug("======ZToMuMuEff: %f", ZMCEff)
        log.debug("======ZToMuMuEff: %f, %f ,%f, %f, %f, %f", ZMCEffBB, ZMCEffBE, ZMCEffEE, ZBBEff, ZBEEff, ZEEEff)

        timeWindow_m = len(goodLSlist) * secPerLS
        delLumi_m = sum(data_run.loc[data_run['ls'].isin(goodLSlist)]['delivered(/pb)'].values)
        deadtime_m = recLumi_m / delLumi_m

        ZXSec = Zyield_m / (ZEff * recLumi_m)
        ZMCXSec = Zyield_m / (ZMCEff * recLumi_m)
        ZRateUncorrected = Zyield_m / (ZEff * timeWindow_m * deadtime_m)
        ZRate = Zyield_m / (ZMCEff * timeWindow_m * deadtime_m)

        ZRate_EStat = [0., 0.]
        for i in (0, 1):
            ZRate_EStat[i] = ZRate * (Zyieldres_m[i+1] / Zyield_m + ZEff_EStat[i] / ZMCEff)

        log.debug("======ZMCXSec: %f", ZMCXSec)
        log.debug("======ZRate: %f", ZRate)

        datestampLow_m = data_run.loc[data_run['ls'] == goodLSlist[0]]['time'].values[0].split(" ")
        datestampUp_m = data_run.loc[data_run['ls'] == goodLSlist[-1]]['time'].values[0].split(" ")

        dateLow_m = ROOT.TDatime(currentYear, int(datestampLow_m[0].split("/")[0]),
                                 int(datestampLow_m[0].split("/")[1]), int(datestampLow_m[1].split(":")[0]),
                                 int(datestampLow_m[1].split(":")[1]), int(datestampLow_m[1].split(":")[2]))
        dateUp_m = ROOT.TDatime(currentYear, int(datestampUp_m[0].split("/")[0]), int(datestampUp_m[0].split("/")[1]),
                                int(datestampUp_m[1].split(":")[0]), int(datestampUp_m[1].split(":")[1]),
                                int(datestampUp_m[1].split(":")[2]))

        log.debug("======beginTime: %s", dateLow_m.Convert())
        log.debug("======endTime: %s", dateUp_m.Convert())

        # Variables to write in csv file
        fillarray.append(fill)
        runarray.append(run)

        ZrateUncorrected.append(ZRateUncorrected)
        Zrate.append(ZRate)
        Zrate_EStatDown.append(ZRate_EStat[0])
        Zrate_EStatUp.append(ZRate_EStat[1])
        ZyieldDel.append(Zyield_m / (ZMCEff * deadtime_m))
        ZyieldRec.append(Zyield_m / ZMCEff)
        ZLumiRec.append(Zyield_m / (ZMCEff * sigma_fid))
        ZLumiDel.append(Zyield_m / (ZMCEff * sigma_fid * deadtime_m))
        ZinstLumiRec.append(Zyield_m / (ZMCEff * sigma_fid * timeWindow_m))
        ZinstLumiDel.append(Zyield_m / (ZMCEff * sigma_fid * deadtime_m * timeWindow_m))

        XSecFid.append(ZMCXSec)
        XSecFidUncorrected.append(ZXSec)

        lumiDel.append(delLumi_m)
        lumiRec.append(recLumi_m)
        instDel.append(delLumi_m / timeWindow_m)
        instRec.append(recLumi_m / timeWindow_m)

        pileUp.append(avgpu_m)
        tdate_begin.append(dateLow_m.Convert())
        tdate_end.append(dateUp_m.Convert())
        windowarray.append(timeWindow_m)
        deadTime.append(deadtime_m)
        beginLS.append(goodLSlist[0])
        endLS.append(goodLSlist[-1])

        Zyield_reco.append(Zyieldres_m[0])
        Zyield_chi2.append(Zyieldres_m[3])
        Zyield_fpr.append(Zyieldres_m[4])

        # Efficiency related
        HLTeffB.append(HLTeffB_m)
        HLTeffE.append(HLTeffE_m)
        SeleffB.append(SeleffB_m)
        SeleffE.append(SeleffE_m)
        GloeffB.append(GloeffB_m)
        GloeffE.append(GloeffE_m)

        HLTeffB_chi2pass.append(HLTeffresB_m[3])
        HLTeffB_chi2fail.append(HLTeffresB_m[4])
        HLTeffE_chi2pass.append(HLTeffresE_m[3])
        HLTeffE_chi2fail.append(HLTeffresE_m[4])
        SeleffB_chi2pass.append(SeleffresB_m[3])
        SeleffB_chi2fail.append(SeleffresB_m[4])
        SeleffE_chi2pass.append(SeleffresE_m[3])
        SeleffE_chi2fail.append(SeleffresE_m[4])
        GloeffB_chi2pass.append(GloeffresB_m[3])
        GloeffB_chi2fail.append(GloeffresB_m[4])
        GloeffE_chi2pass.append(GloeffresE_m[3])
        GloeffE_chi2fail.append(GloeffresE_m[4])

        ZMCeff.append(ZMCEff)
        ZMCeffBB.append(ZMCEffBB)
        ZMCeffBE.append(ZMCEffBE)
        ZMCeffEE.append(ZMCEffEE)

        Zeff.append(ZEff)
        ZBBeff.append(ZBBEff)
        ZBEeff.append(ZBEEff)
        ZEEeff.append(ZEEEff)

        log.info("===Writing per LS CSV file")
        with open(outSubDir + 'byLS.csv', 'ab') as file:

            avgZLumiRec = Zyield_m / (ZMCEff * sigma_fid * len(goodLSlist) * secPerLS)
            avgZLumiDel = Zyield_m / (ZMCEff * sigma_fid * len(goodLSlist) * secPerLS * deadtime_m)

            for ls in goodLSlist:
                time = data_run.loc[data_run['ls'] == ls]['time'].values[0].split(" ")
                ttime = ROOT.TDatime(currentYear, int(time[0].split("/")[0]),
                                                 int(time[0].split("/")[1]), int(time[1].split(":")[0]),
                                                 int(time[1].split(":")[1]), int(time[1].split(":")[2])).Convert()

                file.write("{0}:{1},{2}:{2},{3},{4},{5},ZMonitoring\n".format(run, fill, ls, ttime, avgZLumiDel, avgZLumiRec))

        nMeasurements = nMeasurements + 1

    outCSVDir = outDir + "/csvFiles/"
    if not  os.path.isdir(outCSVDir):
        os.mkdir(outCSVDir)

    ## Write Per Run CSV Files
    log.info("===Writing per Run CSV file")

    result = pd.DataFrame()
    result["fill"] = fillarray
    result["run"] = runarray
    result["tdate_begin"] = tdate_begin
    result["tdate_end"] = tdate_end
    result["ZRate"] = Zrate
    result["ZRate_EStatDown"] = Zrate_EStatDown
    result["ZRate_EStatUp"] = Zrate_EStatUp
    result["ZRateUncorrected"] = ZRateUncorrected
    result["ZYieldDelivered"] = ZyieldDel
    result["ZYieldRecorded"] = ZyieldRec
    result["ZYieldReconstructed"] = Zyield_reco
    result["ZYieldFpr"] = Zyield_fpr
    result["ZLumiDel"] = ZLumiDel
    result["ZLumiRec"] = ZLumiRec
    result["ZLumiDelInst"] = ZinstLumiDel
    result["ZLumiRecInst"] = ZinstLumiRec
    result["XSecFid"] = XSecFid
    result["XSecFidUncorrected"] = XSecFidUncorrected
    result["LumiDel"] = lumiDel
    result["LumiRec"] = lumiRec
    result["LumiDelInst"] = instDel
    result["LumiRecInst"] = instRec
    result["timewindow"] = windowarray
    result["deadtime"] = deadTime
    result["pileUp"] = pileUp
    result["HLTeffB"] = HLTeffB
    result["HLTeffE"] = HLTeffE
    result["SeleffB"] = SeleffB
    result["SeleffE"] = SeleffE
    result["GloeffB"] = GloeffB
    result["GloeffE"] = GloeffE
    result["HLTeffB_chi2pass"] = HLTeffB_chi2pass
    result["HLTeffB_chi2fail"] = HLTeffB_chi2fail
    result["HLTeffE_chi2pass"] = HLTeffE_chi2pass
    result["HLTeffE_chi2fail"] = HLTeffE_chi2fail
    result["SeleffB_chi2pass"] = SeleffB_chi2pass
    result["SeleffB_chi2fail"] = SeleffB_chi2fail
    result["SeleffE_chi2pass"] = SeleffE_chi2pass
    result["SeleffE_chi2fail"] = SeleffE_chi2fail
    result["GloeffB_chi2pass"] = GloeffB_chi2pass
    result["GloeffB_chi2fail"] = GloeffB_chi2fail
    result["GloeffE_chi2pass"] = GloeffE_chi2pass
    result["GloeffE_chi2fail"] = GloeffE_chi2fail
    result["Zeff"] = Zeff
    result["ZBBeff"] = ZBBeff
    result["ZBEeff"] = ZBEeff
    result["ZEEeff"] = ZEEeff
    result["ZMCeff"] = ZMCeff
    result["ZMCeffBB"] = ZMCeffBB
    result["ZMCeffBE"] = ZMCeffBE
    result["ZMCeffEE"] = ZMCeffEE

    with open(outCSVDir + '/csvfile{0}.csv'.format(run), 'w') as file:
        result.to_csv(file, index=False)

## Write Big CSV File
log.info("===Writing overall CSV file")
if args.writeSummaryCSV:
    rateFileList = sorted(glob.glob(outCSVDir + '/csvfile*.csv'))
    df_merged = pd.concat([pd.read_csv(m) for m in rateFileList], ignore_index=True)

    with open(outCSVDir + 'Mergedcsvfile.csv', 'w') as file:
        df_merged.to_csv(file, index=False)
