import ROOT
import os
from array import array
import pandas as pd
import numpy as np
import json
import glob
import logging as log
import argparse
import pdb

from Utils.Utils import to_RootTime
# disable panda warnings when assigning a new column in the dataframe
pd.options.mode.chained_assignment = None

cmsswbase = os.environ['CMSSW_BASE']

parser = argparse.ArgumentParser()
parser.add_argument("-b", "--beginRun", help="first run to analyze [%default]", default=272007)
parser.add_argument("-e", "--endRun", help="analyze stops when comes to this run [%default]", default=1000000)
parser.add_argument('--mcCorrections', default='./Resources/MCCorrections.json', type=str,
                    help='specify .json file with MC corrections for muon correlations')
parser.add_argument("-v", "--verbose", help="increase logging level from INFO to DEBUG", default=False,
                    action="store_true")
parser.add_argument("-c", "--writeSummaryCSV", help="produce merged CSV with all runs", default=True)
parser.add_argument("--dirDQM", help="Directory to the input root files from the DQM Offline module",
                    default="/eos/home-d/dwalter/www/ZCounting/DQM-Offline-2018/")
parser.add_argument("--byLsCSV", help="ByLs csv input generated by testBril.sh",
                    default="/eos/cms/store/group/comm_luminosity/ZCounting/brilcalcFile2018/*csv")
parser.add_argument("--sigTemplates", help="Directory to root file for Z mass template",
                    default=cmsswbase + "/src/ZCounting/Resources/")
parser.add_argument('--ptCut', type=float,
    help='specify lower pt cut on tag and probe muons')
parser.add_argument("-o", "--dirOut", help="where to store the output files", default="./")

args = parser.parse_args()
if args.verbose:
    log.basicConfig(format="%(levelname)s: %(message)s", level=log.DEBUG)
else:
    log.basicConfig(format="%(levelname)s: %(message)s", level=log.INFO)

########## Input configuration ##########
# ByLS csv inputs generated by testBRIL.sh
byLS_filelist = glob.glob(args.byLsCSV)
byLS_filelist.sort(key=os.path.getmtime)
byLS_filename = byLS_filelist[-1]
print("The brilcalc csv file: " + str(byLS_filename))

eosDir = args.dirDQM
mcCorr = args.mcCorrections

# MC inputs: to build MC*Gaussian template for efficiency fitting
mcShapeDir = args.sigTemplates if args.sigTemplates.endswith("/") else args.sigTemplates+"/"

ptCutTag = args.ptCut
ptCutProbe = args.ptCut

outDir = args.dirOut if args.dirOut.endswith("/") else args.dirOut+"/"
if not os.path.isdir(outDir):
    os.mkdir(outDir)

outCSVDir = outDir+"csvFiles/"
if not os.path.isdir(outCSVDir):
    try:
        os.mkdir(outCSVDir)
    except OSError:
        print("directory already exists ...")

########################################

with open(mcCorr) as json_file:
    corr = json.load(json_file)

########### Constant settings ##########
secPerLS = float(23.3)
currentYear = 2017

#from 2017H low PU, w/o PU corrections in pb. |eta| < 2.4,  pt>30
sigma_fid = 610.1401700042975

maximumLS = 2500
LSperMeasurement = 100  # required number of lumi sections per measurement
LumiPerMeasurement = 20  # minimum recorded lumi for one measurement in pb-1

fakerate = 0.01

ZBBAcc = 0.077904  # Fraction of Z events where both muons are in barrel region
ZBEAcc = 0.117200  # barrel and endcap
ZEEAcc = 0.105541  # both endcap

fullAcc = (ZBBAcc + ZBEAcc + ZEEAcc)

ZBBAcc = ZBBAcc / fullAcc
ZBEAcc = ZBEAcc / fullAcc
ZEEAcc = ZEEAcc / fullAcc

########################################
def full_efficiency(effBB, effBE, effEE):
    return effBB * ZBBAcc + effBE * ZBEAcc + effEE * ZEEAcc

def full_uncertainty(effBB_stat, effBE_stat, effEE_stat):
    return [np.sqrt((ZBBAcc * effBB_stat[i]) ** 2 + (ZBEAcc * effBE_stat[i]) ** 2 + (ZEEAcc * effEE_stat[i]) ** 2) for i in range(0,1)]


# log.info("Loading C marco...")	#I think we don't need this
ROOT.gROOT.LoadMacro(os.path.dirname(os.path.realpath(
    __file__)) + "/calculateDataEfficiency.C")  # load function getZyield(...) and calculateDataEfficiency(...)

# turn off graphical output on screen
ROOT.gROOT.SetBatch(True)

log.info("Loading input byls csv...")
byLS_file = open(str(byLS_filename))
byLS_lines = byLS_file.readlines()
byLS_data = pd.read_csv(byLS_filename, sep=',', low_memory=False,
                   skiprows=lambda x: byLS_lines[x].startswith('#') and not byLS_lines[x].startswith('#run'))
log.debug("%s", byLS_data.axes)
log.info("Loading input byls csv DONE...")
# formatting the csv

byLS_data[['run', 'fill']] = byLS_data['#run:fill'].str.split(':', expand=True).apply(pd.to_numeric)
byLS_data['ls'] = byLS_data['ls'].str.split(':', expand=True)[0].apply(pd.to_numeric)
byLS_data = byLS_data.drop(['#run:fill', 'hltpath', 'source'], axis=1)

if 'delivered(/ub)' in byLS_data.columns.tolist():  # convert to /pb
    byLS_data['delivered(/ub)'] = byLS_data['delivered(/ub)'].apply(lambda x: x / 1000000.)
    byLS_data['recorded(/ub)'] = byLS_data['recorded(/ub)'].apply(lambda x: x / 1000000.)
    byLS_data = byLS_data.rename(index=str, columns={'delivered(/ub)': 'delivered(/pb)', 'recorded(/ub)': 'recorded(/pb)'})
elif 'delivered(/fb)' in byLS_data.columns.tolist():  # convert to /pb
    byLS_data['delivered(/fb)'] = byLS_data['delivered(/fb)'].apply(lambda x: x * 1000.)
    byLS_data['recorded(/fb)'] = byLS_data['recorded(/fb)'].apply(lambda x: x * 1000.)
    byLS_data = byLS_data.rename(index=str, columns={'delivered(/fb)': 'delivered(/pb)', 'recorded(/fb)': 'recorded(/pb)'})

# if there are multiple entries of the same ls (for example from different triggers), only keep the one with the highest luminosity.
byLS_data = byLS_data.sort_values(['fill', 'run', 'ls', 'delivered(/pb)', 'recorded(/pb)'])
byLS_data = byLS_data.drop_duplicates(['fill', 'run', 'ls'])

log.info("Looping over runs...")
for run in byLS_data.drop_duplicates('run')['run'].values:

    if run < int(args.beginRun) or run >= int(args.endRun):
        continue

    data_run = byLS_data.loc[byLS_data['run'] == run]
    fill = data_run.drop_duplicates('fill')['fill'].values[0]
    LSlist = data_run.query('ls <= {0}'.format(maximumLS))['ls'].values.tolist()

    # check if run was processed already
    outSubDir = outDir + "Run{0}/".format(run)
    if os.path.isdir(outSubDir):
        log.info("Run %i was already processed, skipping and going to next run", run)
        continue
    os.mkdir(outSubDir)

    log.info("===Running Run %i", run)
    log.info("===Running Fill %i", fill)

    log.debug("===Setting up arrays for output csv...")
    fillarray = array('i')
    runarray = array('i')
    tdate_begin = array('i')
    tdate_end = array('i')
    zDel = array('d')
    zDel_mc = array('d')
    z_relstat = array('d')
    zRate = array('d')
    zRate_mc = array('d')
    zLumi = array('d')
    zLumi_mc = array('d')
    zXSec = array('d')
    zXSec_mc = array('d')
    windowarray = array('d')
    deadTime = array('d')
    beginLS = array('i')
    endLS = array('i')
    pileUp = array('d')
    lumiDel = array('d')
    lumiRec = array('d')

    # Efficiency related
    HLTeffB = array('d')
    HLTeffE = array('d')
    SeleffB = array('d')
    SeleffE = array('d')
    GloeffB = array('d')
    GloeffE = array('d')
    StaeffB = array('d')
    StaeffE = array('d')
    TrkeffB = array('d')
    TrkeffE = array('d')

    HLTeffB_chi2pass = array('d')
    HLTeffB_chi2fail = array('d')
    HLTeffE_chi2pass = array('d')
    HLTeffE_chi2fail = array('d')
    SeleffB_chi2pass = array('d')
    SeleffB_chi2fail = array('d')
    SeleffE_chi2pass = array('d')
    SeleffE_chi2fail = array('d')
    GloeffB_chi2pass = array('d')
    GloeffB_chi2fail = array('d')
    GloeffE_chi2pass = array('d')
    GloeffE_chi2fail = array('d')
    StaeffB_chi2pass = array('d')
    StaeffB_chi2fail = array('d')
    StaeffE_chi2pass = array('d')
    StaeffE_chi2fail = array('d')
    TrkeffB_chi2pass = array('d')
    TrkeffB_chi2fail = array('d')
    TrkeffE_chi2pass = array('d')
    TrkeffE_chi2fail = array('d')

    ZMCeff = array('d')
    ZMCeffBB = array('d')
    ZMCeffBE = array('d')
    ZMCeffEE = array('d')

    Zeff = array('d')
    ZBBeff = array('d')
    ZBEeff = array('d')
    ZEEeff = array('d')

    nMeasurements = 0

    log.info("===Loading input DQMIO.root file...")
    eosFileList = glob.glob(eosDir + '/*/*' + str(run) + '*root')

    if not len(eosFileList) > 0:
        print("The file does not yet exist for run: " + str(run))
        continue
    else:
        eosFile = eosFileList[0]

    print("The file exists: " + str(eosFile) + " for run  " + str(run))
    log.info("===Looping over measurements...")


    while len(LSlist) > 0:  # begin next measurement "m"
        log.debug("Openning DQMIO.root file: %s", eosFile)

        mergeMeasurements = False
        # merge data to one measuement if remaining luminosity is too less for two measuements
        if (sum(data_run.loc[data_run['ls'].isin(LSlist)]['recorded(/pb)'].values) < 1.5 * LumiPerMeasurement):
            mergeMeasurements = True

        # produce goodLSlist with ls that are used for one measurement
        goodLSlist = []
        recLumi_m = 0
        while (recLumi_m < LumiPerMeasurement or len(goodLSlist) < LSperMeasurement or mergeMeasurements) and len(LSlist) > 0:

            goodLSlist.append(LSlist[0])
            recLumi_m += (data_run[data_run['ls'] == LSlist[0]]['recorded(/pb)'].values)[0]
            del LSlist[0]

        ### load histograms
        dqmfile = ROOT.TFile(eosFile)

        def load_histo(name_):
            h_X_ls = dqmfile.Get("DQMData/Run {0}/ZCounting/Run summary/Histograms/{1}".format(run, name_))
            h_X = h_X_ls.ProjectionY("h_mass_{0}_{1}".format(name_, run), goodLSlist[0], goodLSlist[0], "e")
            for ls in goodLSlist[1:]:
                h_X.Add(h_X_ls.ProjectionY("h_mass_{0}_{1}_{2}".format(name_, run, ls), ls, ls, "e"))
            return h_X

        ### compute Z yield #TODO
        #Zyieldres_m = ROOT.getZyield(load_histo("h_mass_yield_Z"), outSubDir, nMeasurements, 0, 0, ptCutTag, ptCutProbe, recLumi_m);

        ### compute muon efficiencies
        HLTeffresB_m = ROOT.calculateDataEfficiency(load_histo("h_mass_HLT_pass_central"), load_histo("h_mass_HLT_fail_central"),
                                                    outSubDir, nMeasurements, "HLT", 0, 2, 5, 2, 5,
                                                    ptCutTag, ptCutProbe, 0, recLumi_m, mcShapeDir + "template_HLT.root")
        HLTeffresE_m = ROOT.calculateDataEfficiency(load_histo("h_mass_HLT_pass_forward"), load_histo("h_mass_HLT_fail_forward"),
                                                    outSubDir, nMeasurements, "HLT", 1, 2, 5, 2, 5,
                                                    ptCutTag, ptCutProbe, 0, recLumi_m, mcShapeDir + "template_HLT.root")

        SeleffresB_m = ROOT.calculateDataEfficiency(load_histo("h_mass_SIT_pass_central"), load_histo("h_mass_SIT_fail_central"),
                                                    outSubDir, nMeasurements, "Sel", 0, 2, 1, 2, 1,
                                                    ptCutTag, ptCutProbe, 0, recLumi_m, mcShapeDir + "template_Sel.root")
        SeleffresE_m = ROOT.calculateDataEfficiency(load_histo("h_mass_SIT_pass_forward"), load_histo("h_mass_SIT_fail_forward"),
                                                    outSubDir, nMeasurements, "Sel", 1, 2, 1, 2, 1,
                                                    ptCutTag, ptCutProbe, 0, recLumi_m, mcShapeDir + "template_Sel.root")

        GloeffresB_m = ROOT.calculateDataEfficiency(load_histo("h_mass_Glo_pass_central"), load_histo("h_mass_Glo_fail_central"),
                                                    outSubDir, nMeasurements, "Glo", 0, 2, 2, 2, 2,
                                                    ptCutTag, ptCutProbe, 0, recLumi_m, mcShapeDir + "template_Glo.root")
        GloeffresE_m = ROOT.calculateDataEfficiency(load_histo("h_mass_Glo_pass_forward"), load_histo("h_mass_Glo_fail_forward"),
                                                    outSubDir, nMeasurements, "Glo", 1, 2, 2, 2, 2,
                                                    ptCutTag, ptCutProbe, 0, recLumi_m, mcShapeDir + "template_Glo.root")

        HLTeffB_m = HLTeffresB_m[0]
        HLTeffE_m = HLTeffresE_m[0]
        SeleffB_m = SeleffresB_m[0]
        SeleffE_m = SeleffresE_m[0]
        GloeffB_m = GloeffresB_m[0]
        GloeffE_m = GloeffresE_m[0]

        # ZtoMuMu efficiency purely from data
        ZBBEff = (GloeffB_m * GloeffB_m * SeleffB_m * SeleffB_m * (1 - (1 - HLTeffB_m) * (1 - HLTeffB_m)))
        ZBEEff = (GloeffB_m * GloeffE_m * SeleffB_m * SeleffE_m * (1 - (1 - HLTeffB_m) * (1 - HLTeffE_m)))
        ZEEEff = (GloeffE_m * GloeffE_m * SeleffE_m * SeleffE_m * (1 - (1 - HLTeffE_m) * (1 - HLTeffE_m)))

        # Statistic Uncertainties (low,high) error propagation
        ZBBEff_EStat = [0., 0.]
        ZBEEff_EStat = [0., 0.]
        ZEEEff_EStat = [0., 0.]
        for i in (1, 2):
            ZBBEff_EStat[i - 1] = 2 * ZBBEff * np.sqrt(
                (GloeffresB_m[i] / GloeffB_m) ** 2 +
                (SeleffresB_m[i] / SeleffB_m) ** 2 +
                ((1 - HLTeffB_m) / (1 - (1 - HLTeffB_m) ** 2) * HLTeffresB_m[i]) ** 2
            )
            ZEEEff_EStat[i - 1] = 2 * ZEEEff * np.sqrt(
                (GloeffresE_m[i] / GloeffE_m) ** 2 +
                (SeleffresE_m[i] / SeleffE_m) ** 2 +
                ((1 - HLTeffE_m) / (1 - (1 - HLTeffE_m) ** 2) * HLTeffresE_m[i]) ** 2
            )
            ZBEEff_EStat[i - 1] = ZBEEff * np.sqrt(
                (GloeffresB_m[i] / GloeffB_m) ** 2 +
                (GloeffresE_m[i] / GloeffE_m) ** 2 +
                (SeleffresB_m[i] / SeleffB_m) ** 2 +
                (SeleffresE_m[i] / SeleffE_m) ** 2 +
                ((1 - HLTeffE_m) / (1 - (1 - HLTeffB_m) * (1 - HLTeffE_m)) * HLTeffresB_m[i]) ** 2 +
                ((1 - HLTeffB_m) / (1 - (1 - HLTeffB_m) * (1 - HLTeffE_m)) * HLTeffresE_m[i]) ** 2
            )

        ZEff = full_efficiency(ZBBEff, ZBEEff, ZEEEff)
        ZEff_stat = full_uncertainty(ZBBEff_EStat, ZBEEff_EStat, ZEEEff_EStat)

        # --- per ls dataframe, one per measurement
        data_m = data_run.loc[data_run['ls'].isin(goodLSlist)]
        data_m['effBB_mc'] = ZBBEff - (data_m['avgpu'] * corr['BB_a'] + corr['BB_b'])
        data_m['effBE_mc'] = ZBEEff - (data_m['avgpu'] * corr['BE_a'] + corr['BE_b'])
        data_m['effEE_mc'] = ZEEEff - (data_m['avgpu'] * corr['EE_a'] + corr['EE_b'])

        data_m['eff_mc'] = full_efficiency(data_m['effBB_mc'], data_m['effBE_mc'], data_m['effEE_mc'])

        hist_Zyield = dqmfile.Get("DQMData/Run {0}/ZCounting/Run summary/Histograms/h_mass_yield_Z".format(run))
        hist_ZyieldBB = dqmfile.Get("DQMData/Run {0}/ZCounting/Run summary/Histograms/h_yieldBB_Z".format(run))
        hist_ZyieldEE = dqmfile.Get("DQMData/Run {0}/ZCounting/Run summary/Histograms/h_yieldEE_Z".format(run))

        data_m['zYield'] = data_m['ls'].apply(lambda ls: hist_Zyield.ProjectionY("", ls, ls, "e").GetEntries() * (1.-fakerate))
        data_m['zYieldBB'] = data_m['ls'].apply(lambda ls: hist_ZyieldBB.GetBinContent(ls) * (1.-fakerate))
        data_m['zYieldEE'] = data_m['ls'].apply(lambda ls: hist_ZyieldEE.GetBinContent(ls) * (1.-fakerate))

        data_m['zDel'] = data_m['zYield'] / ZEff
        data_m['zDelBB'] = data_m['zYieldBB'] / ZBBEff
        data_m['zDelEE'] = data_m['zYieldEE'] / ZEEEff

        data_m['zDel_mc'] = data_m['zYield'] / data_m['eff_mc']
        data_m['zDelBB_mc'] = data_m['zYieldBB'] / (data_m['effBB_mc'])
        data_m['zDelEE_mc'] = data_m['zYieldEE'] / (data_m['effEE_mc'])

        data_m['z_relstat'] = (1./np.sqrt(data_m['zYield']) + max(ZEff_stat)/ZEff).replace(np.inf, 1.)

        data_m['time'] = data_m['time'].apply(lambda x: to_RootTime(x,currentYear))
        data_m['run'] = np.ones(len(data_m),dtype='int') * run
        data_m['fill'] = np.ones(len(data_m),dtype='int') * fill

        with open(outCSVDir + 'csvfile{0}_{1}.csv'.format(run, nMeasurements), 'w') as file:
            data_m.to_csv(file, index=False)

        # --- per measurement dataframe
        delLumi_m = data_m['delivered(/pb)'].sum()
        recLumi_m = data_m['recorded(/pb)'].sum()
        deadtime_m = recLumi_m / delLumi_m
        timeWindow_m = len(goodLSlist) * secPerLS

        zDel_m = data_m['zDel'].sum()
        zDel_mc_m = data_m['zDel_mc'].sum()
        z_relstat_m = 1./max(np.sqrt(data_m['zYield'].sum()),1.) + max(ZEff_stat)/ZEff
        zRate_m = zDel_m / (timeWindow_m * deadtime_m)
        zRate_mc_m = zDel_mc_m / (timeWindow_m * deadtime_m)
        zLumi_m = zDel_m / sigma_fid
        zLumi_mc_m = zDel_mc_m / sigma_fid
        zXSec_m = zDel_m / recLumi_m
        zXSec_mc_m = zDel_mc_m / recLumi_m

        dateLow_m = to_RootTime(data_run.loc[data_run['ls'] == goodLSlist[0]]['time'].values[0])
        dateUp_m = to_RootTime(data_run.loc[data_run['ls'] == goodLSlist[-1]]['time'].values[0])

        # Variables to write in csv file
        fillarray.append(fill)
        runarray.append(run)

        zDel.append(zDel_m)
        zDel_mc.append(zDel_mc_m)
        z_relstat.append(z_relstat_m)
        zRate.append(zRate_m)
        zRate_mc.append(zRate_mc_m)
        zLumi.append(zLumi_m)
        zLumi_mc.append(zLumi_mc_m)
        zXSec.append(zXSec_m)
        zXSec_mc.append(zXSec_mc_m)

        lumiDel.append(delLumi_m)
        lumiRec.append(recLumi_m)

        pileUp.append(data_m['avgpu'].mean())
        tdate_begin.append(dateLow_m)
        tdate_end.append(dateUp_m)
        windowarray.append(timeWindow_m)
        deadTime.append(deadtime_m)
        beginLS.append(goodLSlist[0])
        endLS.append(goodLSlist[-1])

        # Efficiency related
        HLTeffB.append(HLTeffB_m)
        HLTeffE.append(HLTeffE_m)
        SeleffB.append(SeleffB_m)
        SeleffE.append(SeleffE_m)
        GloeffB.append(GloeffB_m)
        GloeffE.append(GloeffE_m)

        HLTeffB_chi2pass.append(HLTeffresB_m[3])
        HLTeffB_chi2fail.append(HLTeffresB_m[4])
        HLTeffE_chi2pass.append(HLTeffresE_m[3])
        HLTeffE_chi2fail.append(HLTeffresE_m[4])
        SeleffB_chi2pass.append(SeleffresB_m[3])
        SeleffB_chi2fail.append(SeleffresB_m[4])
        SeleffE_chi2pass.append(SeleffresE_m[3])
        SeleffE_chi2fail.append(SeleffresE_m[4])
        GloeffB_chi2pass.append(GloeffresB_m[3])
        GloeffB_chi2fail.append(GloeffresB_m[4])
        GloeffE_chi2pass.append(GloeffresE_m[3])
        GloeffE_chi2fail.append(GloeffresE_m[4])

        ZMCeff.append(data_m['eff_mc'].mean())
        ZMCeffBB.append(data_m['effBB_mc'].mean())
        ZMCeffBE.append(data_m['effBE_mc'].mean())
        ZMCeffEE.append(data_m['effEE_mc'].mean())

        Zeff.append(ZEff)
        ZBBeff.append(ZBBEff)
        ZBEeff.append(ZBEEff)
        ZEEeff.append(ZEEEff)

        nMeasurements = nMeasurements + 1


    ## Write per measurement csv file - one per run
    log.info("===Writing per Run CSV file")

    result = pd.DataFrame()
    result["fill"] = fillarray
    result["run"] = runarray
    result["tdate_begin"] = tdate_begin
    result["tdate_end"] = tdate_end
    result["zDel"] = zDel
    result["zDel_mc"] = zDel_mc
    result["z_relstat"] = z_relstat
    result["zRate"] = zRate
    result["zRate_mc"] = zRate_mc
    result["zLumi"] = zLumi
    result["zLumi_mc"] = zLumi_mc
    result["zXSec"] = zXSec
    result["zXSec_mc"] = zXSec_mc
    result["lumiDel"] = lumiDel
    result["lumiRec"] = lumiRec
    result["timewindow"] = windowarray
    result["deadtime"] = deadTime
    result["pileUp"] = pileUp
    result["HLTeffB"] = HLTeffB
    result["HLTeffE"] = HLTeffE
    result["SeleffB"] = SeleffB
    result["SeleffE"] = SeleffE
    result["GloeffB"] = GloeffB
    result["GloeffE"] = GloeffE
    result["HLTeffB_chi2pass"] = HLTeffB_chi2pass
    result["HLTeffB_chi2fail"] = HLTeffB_chi2fail
    result["HLTeffE_chi2pass"] = HLTeffE_chi2pass
    result["HLTeffE_chi2fail"] = HLTeffE_chi2fail
    result["SeleffB_chi2pass"] = SeleffB_chi2pass
    result["SeleffB_chi2fail"] = SeleffB_chi2fail
    result["SeleffE_chi2pass"] = SeleffE_chi2pass
    result["SeleffE_chi2fail"] = SeleffE_chi2fail
    result["GloeffB_chi2pass"] = GloeffB_chi2pass
    result["GloeffB_chi2fail"] = GloeffB_chi2fail
    result["GloeffE_chi2pass"] = GloeffE_chi2pass
    result["GloeffE_chi2fail"] = GloeffE_chi2fail
    result["Zeff"] = Zeff
    result["ZBBeff"] = ZBBeff
    result["ZBEeff"] = ZBEeff
    result["ZEEeff"] = ZEEeff
    result["ZMCeff"] = ZMCeff
    result["ZMCeffBB"] = ZMCeffBB
    result["ZMCeffBE"] = ZMCeffBE
    result["ZMCeffEE"] = ZMCeffEE

    with open(outCSVDir + '/csvfile{0}.csv'.format(run), 'w') as file:
        result.to_csv(file, index=False)

## Write big per measurement csv file
log.info("===Writing overall CSV file")
if args.writeSummaryCSV:
    rateFileList = sorted(glob.glob(outCSVDir + '/csvfile??????.csv'))
    df_merged = pd.concat([pd.read_csv(m) for m in rateFileList], ignore_index=True)

    with open(outCSVDir + 'Mergedcsvfile_perMeasurement.csv', 'w') as file:
        df_merged.to_csv(file, index=False)

    rateFileList = sorted(glob.glob(outCSVDir + '/csvfile*_*.csv'))
    df_merged = pd.concat([pd.read_csv(m) for m in rateFileList], ignore_index=True)

    with open(outCSVDir + 'Mergedcsvfile_perLS.csv', 'w') as file:
        df_merged.to_csv(file, index=False)

log.info("===Done")
