#!/usr/bin/env python3
import logging as log

import argparse
import ROOT
import os
import pandas as pd
import pdb # python debugger might be helpful (see https://docs.python.org/3/library/pdb.html)

from python.utils import load_input_csv, getFileName, get_ls_for_next_measurement, load_histogram, writeSummaryCSV

ROOT.gROOT.SetBatch(True) # disable root prompts

os.sys.path.append(os.path.expandvars('$CMSSW_BASE/src/ZCounting/'))
from ZUtils.python.utils import to_RootTime


parser = argparse.ArgumentParser()
parser.add_argument("-b", "--beginRun", help="First run to analyze [%(default)s]", type=int, default=272007)
parser.add_argument("-e", "--endRun", help="Analyze stops when comes to this run [%(default)s]", type=int, default=1000000)
parser.add_argument("-i", "--dirDQM", help="Directory to the input root files from the DQM Offline module", required=True)
parser.add_argument("--byLsCSV", help="ByLs csv input generated by brilcalc", default="default")
parser.add_argument("--sigTemplates", default="default", type=str,
    help="Choose one of the options for signal model (MC, MCxGaus, MCxCB, BW, BWxCB, BWxGaus). Default is MCxGaus")
parser.add_argument("--bkgTemplates", default="default", type=str,
    help="Choose one of the options for background model (Exp, Quad, QuadPlusExp, CMSShape, Das). Default is CMSShape")
parser.add_argument("-o", "--dirOut", help="where to store the output files", default="./")
args = parser.parse_args()

log.info("Initialial settings")

dirDQM = args.dirDQM
dirOut = args.dirOut
beginRun = args.beginRun
endRun = args.endRun
byLsCSV = args.byLsCSV # contains the reference luminosity

dirOutCSV = dirOut+"/csvFiles/"  # folder to store the output csv files

secPerLS = 23.3

currentYear = 2022

etaRegion = "I" # define eta region. Currently supported are "B" (Barrel) and "I" (Inclusive)

nBinsMass = 50  # number of bins in histograms to fit
massLo = 66     # lower boundary for fit range
massHi = 116    # upper boundary for fit range

npvLo = 0.5     # lower boundary for npv histogram
npvHi = 100.5   # upper boundary for npv histogram

maximumLS = 2500        # maximum luminosity block number that is stored in the DQM histograms
minLSperRun = 20        # only consider runs with at least 20 luminosity blocks
LumiPerMeasurement = 10 # minimum amount of luminosity required to perform one measurement
ptCut = 27              # pT cut on muons, only for labeling

# |eta| cut on muons, only for labeling 
if etaRegion == "B":
    etaCut = 0.9
else:
    etaCut = 2.4            

# signal model
sigTemplates = "/eos/cms/store/group/comm_luminosity/ZCounting/2022/SignalTemplates/ZCountingAll-V01-Winter22-DYJetsToLL_M_50_LO.root"
if args.sigTemplates == "MCxGauss" or args.sigTemplates == "default":
    sigModel = 2 # MC, folding with gauss
elif args.sigTemplates == "MC":
    sigModel = 4 # MC, no folding
elif args.sigTemplates == "BW":
    sigModel = 3 # BW, no folding
elif args.sigTemplates == "BWxCB":
    sigModel = 1 # BW, folding with crystal ball
elif args.sigTemplates == "BWxGaus":
    sigModel = 5 # BW, folding with gauss
elif args.sigTemplates == "MCxCB":
    sigModel = 6 # MC, folding with crystal ball
else:
    log.warning(f"signal model {args.sigTemplates} unknown! exit()")
    exit()

# background model
if args.bkgTemplates == "CMSShape" or args.bkgTemplates == "default" :
    bkgModel = 6
elif args.bkgTemplates == "Exp":
    bkgModel = 1
elif args.bkgTemplates == "Quad":
    bkgModel = 2
elif args.bkgTemplates == "QuadPlusExp":
    bkgModel = 3
elif args.bkgTemplates == "Das":
    bkgModel = 4
else:
    log.warning(f"background model {args.bkgTemplates} unknown! exit()")
    exit()

log.info(f"Load the ByLS csv file {byLsCSV} with information of the reference luminosity")
byLS_data = load_input_csv(byLsCSV)

if not os.path.isdir(dirOut):
    log.info(f"create output directory {dirOut}")
    os.mkdir(dirOut)

dirMacros = os.path.dirname(os.path.realpath(__file__)) + "/calculateDataEfficiency.C"
log.info(f"Load root macros from {dirMacros} ...")
ROOT.gROOT.LoadMacro(dirMacros)
ROOT.set_massRange(massLo, massHi, nBinsMass)
ROOT.set_npvRange(npvLo, npvHi)
ROOT.set_ptCut(ptCut)
ROOT.set_etaCut(etaCut)
ROOT.set_energy(13.6)

results = []
# loop over all runs to be fit
log.info("Looping over runs ...")
for run, byLS_run in byLS_data.groupby('run'):

    if run < int(args.beginRun) or run >= int(args.endRun):
        continue
    
    fill = byLS_run.drop_duplicates('fill')['fill'].values[0]
    LSlist = byLS_run.query(f'ls <= {maximumLS}')['ls'].values.tolist()
    Lumilist = byLS_run.loc[byLS_run['ls'].isin(LSlist)]['recorded(/pb)'].values.tolist()

    # Consider only runs with a minimum number of LS
    if len(LSlist) <= minLSperRun:
        log.info(f"Skip run {run} since it only has {len(LSlist)} lumisections")
        continue
    
    log.info(f"Now at run {run}")
    fileName = getFileName(dirDQM,run)
    
    log.info(f"Found file `{fileName}`")

    dirOutSub = f"{dirOut}/Run{run}"
    if not os.path.isdir(dirOutSub):
        log.info(f"create output directory {dirOutSub}")
        os.mkdir(dirOutSub)
        
    ROOT.set_output(dirOutSub)

    log.info("Looping over measurements ...")    
    for m, goodLSlist in enumerate(
        get_ls_for_next_measurement(lumisections=LSlist, luminosities=Lumilist, lumiPerMeasurement=LumiPerMeasurement)
    ):
                
        # create datafram byLS for measurement
        byLS_m = byLS_run.loc[byLS_run['ls'].isin(goodLSlist)]
        recLumi = byLS_m['recorded(/pb)'].sum()
        
        ROOT.set_luminosity(recLumi)
        
        log.info(f"Measurement {m} with {recLumi}/pb lumi from {len(goodLSlist)} lumisections")
            
        # get histograms from good lumisections
        log.info("Load histograms ...")

        # get histogram with primary vertex distribution
        hPV = load_histogram("h_npv", fileName, goodLSlist, run=run, prefix="", suffix="new", pileup=True)

        # get histograms binned in mass
        def load(name_):
            return load_histogram(name_, fileName, goodLSlist, run=run, 
                MassBin=nBinsMass, MassMin=massLo, MassMax=massHi, 
                # prefix=f"DQMData/Run {run}/ZCounting/Run summary/Histograms/", 
                prefix="", 
                suffix="new")

        # load histograms for hlt efficiency and Z yield
        h_hlt2 = load(f"h_mass_2HLT")
        h_hlt1 = load(f"h_mass_1HLT")
        
        ROOT.calculateHLTEfficiencyAndYield(h_hlt2, h_hlt1, m, etaRegion, sigModel, bkgModel, sigModel, bkgModel, hPV, sigTemplates)
        
        # load histograms for selection efficiency
        h_sel_fail = load(f"h_mass_SIT_fail")
        
        h_sel_pass = h_hlt2.Clone()
        h_sel_pass.Scale(2)
        h_sel_pass.Add(h_hlt1)
        
        ROOT.calculateDataEfficiency(h_sel_pass, h_sel_fail, m, "Sel", etaRegion, sigModel, bkgModel, sigModel, bkgModel, hPV, sigTemplates)

        # load histograms for global muon efficiency
        h_glo_pass = load(f"h_mass_Glo_pass")
        h_glo_fail = load(f"h_mass_Glo_fail")

        ROOT.calculateDataEfficiency(h_glo_pass, h_glo_fail, m, "Glo", etaRegion, sigModel, bkgModel, sigModel, bkgModel, hPV, sigTemplates)

        # load histograms for standalone muon efficiency
        h_sta_pass = load(f"h_mass_Sta_pass")
        h_sta_fail = load(f"h_mass_Sta_fail")

        ROOT.calculateDataEfficiency(h_sta_pass, h_sta_fail, m, "Sta", etaRegion, sigModel, bkgModel, sigModel, bkgModel, hPV, sigTemplates)

        
        log.info("Load fit results ...")
        
        def load_from_workspace(type_eff):
            if type_eff == "yield":
                filename_eff = f"{dirOutSub}/workspace_{etaRegion}_{m}.root"
            else:
                filename_eff = f"{dirOutSub}/workspace_{type_eff}_{etaRegion}_{m}.root"
                
            if not os.path.isfile(filename_eff):
                log.warning(f"fit result {filename_eff} does not exist! exit")
                return None
                
            tfile_eff = ROOT.TFile(filename_eff,"READ")
            workspace = tfile_eff.Get("workspace")
            eff = workspace.var("eff").getVal()
                        
            if type_eff != "yield":
                return eff
            else:
                nsig = workspace.var("Nsig").getVal()
                return nsig, eff
                
        
        recoZ, effHLT = load_from_workspace("yield")
        effSel = load_from_workspace("Sel")
        effGlo = load_from_workspace("Glo")
        effSta = load_from_workspace("Sta")
        
        # trigger efficiency that at least one muon passes the trigger
        # effTrigger = (1 - (1-effHLT)**2 )
        # reconstructed number of Zs, do not perform fit but assume 1% background
        # recoZ = h_mass_yield_Z.Integral() * 0.99
        # delivered number of Zs given by efficiency corrected number of reconstructed Zs
        # delZ = recoZ / (effTrigger * effSel**2 * effGlo**2)
        
        delZ = recoZ / (effSel**2 * effGlo**2 * effSta**2)

        log.info(f"Delivered number of Zs = {delZ}")

        # deadtime during the good lumisections
        deadtime = byLS_m['recorded(/pb)'].sum() / byLS_m['delivered(/pb)'].sum()
        
        # amount of seconds where we measured the Z counts (during good lumisections)
        timewindow = len(byLS_m)*secPerLS
        # convert time string to standard time_t format (which is a UInt_t) 
        beginTime = to_RootTime(byLS_m['time'][0], currentYear)
        endTime = to_RootTime(byLS_m['time'][-1], currentYear) + int(secPerLS)
        
        # total time window from the beginning of the first to the end of the last lumisection
        totaltimewindow = endTime - beginTime
        # convert standard time_t format back to time string
        beginTime = ROOT.TDatime(beginTime).AsSQLString().replace("-","/")[2:]
        endTime = ROOT.TDatime(endTime).AsSQLString().replace("-","/")[2:]
        
        result = {
            "fill": fill,
            "run": run,
            "measurement": m,
            "beginTime": beginTime,
            "endTime": endTime,
            "deadtime": deadtime,            
            "timewindow": timewindow,
            "totaltimewindow": totaltimewindow,
            "delLumi": byLS_m['delivered(/pb)'].sum(),
            "recLumi": byLS_m['recorded(/pb)'].sum(),
            "instDelLumi": byLS_m['delivered(/pb)'].sum() / timewindow,
            "instRecLumi": byLS_m['recorded(/pb)'].sum() / timewindow,
            "pileUp": byLS_m['avgpu'].mean(),
            "effHLT": effHLT,
            "effSel": effSel,
            "effGlo": effGlo,
            "effSta": effSta,
            # delivered Z rate (corrected for deadtime and extrapolatet to the total time window)
            "delZCount": delZ / deadtime * (totaltimewindow / timewindow),    
            # instantaneous delivered Z rate (corrected for deadtime)
            "ZRate": delZ / (timewindow * deadtime)   
            
        }
        results.append(result)

    # make one dataframe with information of the measurements
    df_results = pd.concat([pd.DataFrame([result]) for result in results], ignore_index=True, sort=False)

    if not os.path.isdir(dirOutCSV):
        os.mkdir(dirOutCSV)
        
    # write one single csv file for each run             
    with open(f'{dirOutCSV}/csvfile{run}.csv', 'w') as file:
        df_results.to_csv(file, index=False)

# write one large file containg full information of all measurements
writeSummaryCSV(dirOutCSV, outName=f"Mergedcsvfile_{etaCut}", writeByLS=False)

# write one large file containg CMS information of all measurements - to share with ATLAS
writeSummaryCSV(dirOutCSV, outName=f"cms_{etaCut}", writeByLS=False, 
    keys=["fill","beginTime","endTime","ZRate","instDelLumi","delLumi","delZCount"])

log.info(" ===Done")
