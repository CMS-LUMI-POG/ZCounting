#!/usr/bin/env python3
import logging as log

import argparse
import ROOT
import os
import pandas as pd
import pdb # python debugger might be helpful (see https://docs.python.org/3/library/pdb.html)
import datetime
from python.utils import load_input_csv, getFileName, get_ls_for_next_measurement, load_histogram, to_DateTime, writeSummaryCSV, getCorrelationIO

ROOT.gROOT.SetBatch(True) # disable root prompts

parser = argparse.ArgumentParser()
parser.add_argument("-b", "--beginRun", help="First run to analyze [%(default)s]", type=int, default=272007)
parser.add_argument("-e", "--endRun", help="Analyze stops when comes to this run [%(default)s]", type=int, default=1000000)
parser.add_argument("-i", "--dirDQM", help="Directory to the input root files from the DQM Offline module", required=True)
parser.add_argument("--byLsCSV", help="ByLs csv input generated by brilcalc", default="default")
parser.add_argument('--mcCorrections', default="default", type=str,
    help='specify file with MC corrections for muon correlations')
parser.add_argument("--sigTemplates", default="default", type=str,
    help="Choose one of the options for signal model (MC, MCxGaus, MCxCB, BW, BWxCB, BWxGaus). Default is MCxGaus")
parser.add_argument("--bkgTemplates", default="default", type=str,
    help="Choose one of the options for background model (Exp, Quad, QuadPlusExp, CMSShape, Das). Default is CMSShape")
parser.add_argument('--ptCut', type=float, default=27.,
    help='specify lower pt cut on tag and probe muons')
parser.add_argument('--etaRegion', type=str, default="I",
    help='specify |eta| region for tag and probe muons. Either "B" (Barrel) or "E" (Endcap) ')
parser.add_argument('--mass', nargs=3, metavar=('LOW', 'HIGH', 'NUMBER'), default=(66,116,50), type=int,
    help='specify mass range for tag and probe muon pairs')
parser.add_argument('--LumiPerMeasurement', default=20, type=float,
    help='specify amount of luminosity per measurement in pb-1')
parser.add_argument("-o", "--dirOut", help="where to store the output files", default="./")
args = parser.parse_args()

log.info("Initialial settings")

dirDQM = args.dirDQM
dirOut = args.dirOut
beginRun = args.beginRun
endRun = args.endRun
byLsCSV = args.byLsCSV # contains the reference luminosity

# Different factorization schemes implemented as "modes", possible modes:
#   "DQMData": histograms from DQMOffline as implemented in cmssw
#   "LUM-21-001": as implemented in Run 2 paper
mode = "DQMData"

dirOutCSV = dirOut+"/csvFiles/"  # folder to store the output csv files

# path to the file with the correlation factors between inner and outer track
if args.mcCorrections:
    correlationsIO = "/eos/cms/store//group/comm_luminosity/ZCounting/2022/CorrelationFactors/cMu_nPV_2022.root"
else:
    correlationsIO = args.mcCorrections

secPerLS = 23.3
currentYear = 2022

nBinsMass = int(args.mass[2])  # number of bins in histograms to fit  ## V02: 120 ATLAS 100
massLo = int(args.mass[0])     # lower boundary for fit range          ## V02: 60  ATLAS  66
massHi =int(args.mass[1])    # upper boundary for fit range          ## V02: 120 ATLAS 116

npvLo = 0.5     # lower boundary for npv histogram
npvHi = 100.5   # upper boundary for npv histogram

maximumLS = 2500        # maximum luminosity block number that is stored in the DQM histograms
minLSperRun = 20        # only consider runs with at least 20 luminosity blocks
LumiPerMeasurement = args.LumiPerMeasurement # minimum amount of luminosity required to perform one measurement
ptCut = args.ptCut              # pT cut on muons, only for labeling  ## V02: 25  ATLAS 27
etaRegion = args.etaRegion

# |eta| cut on muons, only for labeling 
if etaRegion == "B":
    etaCut = 0.9
elif etaRegion == "I":
    etaCut = 2.4
else:  
    log.warning(f"Region `{args.etaRegion}` not defined! exit()")
    exit()

# signal model
sigTemplates = "/eos/cms/store/group/comm_luminosity/ZCounting/2022/SignalTemplates/ZCountingAll-V01-Winter22-DYJetsToLL_M_50_LO.root"
if args.sigTemplates == "MCxGauss" or args.sigTemplates == "default":
    sigModel = 2 # MC, folding with gauss
elif args.sigTemplates == "MC":
    sigModel = 4 # MC, no folding
elif args.sigTemplates == "BW":
    sigModel = 3 # BW, no folding
elif args.sigTemplates == "BWxCB":
    sigModel = 1 # BW, folding with crystal ball
elif args.sigTemplates == "BWxGaus":
    sigModel = 5 # BW, folding with gauss
elif args.sigTemplates == "MCxCB":
    sigModel = 6 # MC, folding with crystal ball
else:
    log.warning(f"signal model {args.sigTemplates} unknown! exit()")
    exit()

# background model
if args.bkgTemplates == "CMSShape" or args.bkgTemplates == "default" :
    bkgModel = 6
elif args.bkgTemplates == "Exp":
    bkgModel = 1
elif args.bkgTemplates == "Quad":
    bkgModel = 2
elif args.bkgTemplates == "QuadPlusExp":
    bkgModel = 3
elif args.bkgTemplates == "Das":
    bkgModel = 4
else:
    log.warning(f"background model {args.bkgTemplates} unknown! exit()")
    exit()

log.info(f"Load the ByLS csv file {byLsCSV} with information of the reference luminosity")
byLS_data = load_input_csv(byLsCSV)

if not os.path.isdir(dirOut):
    log.info(f"create output directory {dirOut}")
    os.mkdir(dirOut)

dirMacros = os.path.dirname(os.path.realpath(__file__)) + "/calculateDataEfficiency.C"
log.info(f"Load root macros from {dirMacros} ...")
ROOT.gROOT.LoadMacro(dirMacros)
ROOT.set_massRange(massLo, massHi, nBinsMass)
ROOT.set_npvRange(npvLo, npvHi)
ROOT.set_ptCut(ptCut)
ROOT.set_etaCut(etaCut)
ROOT.set_energy(13.6)

results = []
# loop over all runs to be fit
log.info("Looping over runs ...")
for run, byLS_run in byLS_data.groupby('run'):

    if run < int(args.beginRun) or run >= int(args.endRun):
        continue
    
    fill = byLS_run.drop_duplicates('fill')['fill'].values[0]
    LSlist = byLS_run.query(f'ls <= {maximumLS}')['ls'].values.tolist()
    Lumilist = byLS_run.loc[byLS_run['ls'].isin(LSlist)]['recorded(/pb)'].values.tolist()

    # Consider only runs with a minimum number of LS
    if len(LSlist) <= minLSperRun:
        log.info(f"Skip run {run} since it only has {len(LSlist)} lumisections")
        continue
    
    log.info(f"Now at run {run}")
    fileName = getFileName(dirDQM,run)
    
    log.info(f"Found file `{fileName}`")

    dirOutSub = f"{dirOut}/Run{run}"
    if not os.path.isdir(dirOutSub):
        log.info(f"create output directory {dirOutSub}")
        os.mkdir(dirOutSub)
        
    ROOT.set_output(dirOutSub)

    log.info("Looping over measurements ...")    
    for m, goodLSlist in enumerate(
        get_ls_for_next_measurement(lumisections=LSlist, luminosities=Lumilist, lumiPerMeasurement=LumiPerMeasurement)
    ):
                
        # create datafram byLS for measurement
        byLS_m = byLS_run.loc[byLS_run['ls'].isin(goodLSlist)]
        recLumi = byLS_m['recorded(/pb)'].sum()
        
        ROOT.set_luminosity(recLumi)
        
        log.info(f"Measurement {m} with {recLumi}/pb lumi from {len(goodLSlist)} lumisections")
            
        # get histograms from good lumisections
        log.info("Load histograms ...")
        if mode == "DQMData":
            prefix=f"DQMData/Run {run}/ZCounting/Run summary/Histograms/"
        else:
            prefix=""

        # get histogram with primary vertex distribution
        hPV = load_histogram("h_npv", fileName, goodLSlist, run=run, prefix=prefix, suffix="new", pileup=True)

        #if (m==4)
        #    continue
        
        # get histograms binned in mass
        def load(name_):
            return load_histogram(name_, fileName, goodLSlist, run=run, 
                MassBin=nBinsMass, MassMin=massLo, MassMax=massHi, 
                prefix=prefix, 
                suffix="new")

        # load histograms for hlt efficiency and Z yield
        h_hlt2 = load(f"h_mass_2HLT_BB")
        h_hlt1 = load(f"h_mass_1HLT_BB")

        if etaRegion == "I":
            h_hlt2.Add(load(f"h_mass_2HLT_BE"))
            h_hlt2.Add(load(f"h_mass_2HLT_EE"))

            h_hlt1.Add(load(f"h_mass_1HLT_BE"))
            h_hlt1.Add(load(f"h_mass_1HLT_EE"))

            ROOT.calculateHLTEfficiencyAndYield(h_hlt2, h_hlt1, m, "I", sigModel, bkgModel, sigModel, bkgModel, hPV, sigTemplates)   # To run for the etaRegion=I
        else:
            ROOT.calculateHLTEfficiencyAndYield(h_hlt2, h_hlt1, m, "BB", sigModel, bkgModel, sigModel, bkgModel, hPV, sigTemplates)  # To run for the etaRegion=B
        
        # load histograms with probes that fail selection, for selection efficiency
        h_sel_fail = load(f"h_mass_SIT_fail_BB")

        if etaRegion == "I":
            h_sel_fail.Add(load(f"h_mass_SIT_fail_BE"))
            h_sel_fail.Add(load(f"h_mass_SIT_fail_EE"))
        
        ROOT.getZyield(h_sel_fail, m, "Sel", etaRegion, sigModel, bkgModel, 0, sigTemplates, hPV)

        if mode == "LUM-21-001":

            # load histograms for global muon efficiency
            h_glo_pass = load(f"h_mass_Glo_pass_BB")
            h_glo_fail = load(f"h_mass_Glo_fail_BB")

            if etaRegion == "I":
                h_glo_pass.Add(load(f"h_mass_Glo_pass_BE"))
                h_glo_pass.Add(load(f"h_mass_Glo_pass_EE"))
                h_glo_fail.Add(load(f"h_mass_Glo_fail_BE"))
                h_glo_fail.Add(load(f"h_mass_Glo_fail_EE"))

            ROOT.calculateDataEfficiency(h_glo_pass, h_glo_fail, m, "Glo", etaRegion, sigModel, bkgModel, sigModel, bkgModel, hPV, sigTemplates)

            # load histograms for standalone muon efficiency
            h_sta_pass = load(f"h_mass_Sta_pass_BB")
            h_sta_fail = load(f"h_mass_Sta_fail_BB")

            if etaRegion == "I":
                h_sta_pass.Add(load(f"h_mass_Sta_pass_BE"))
                h_sta_pass.Add(load(f"h_mass_Sta_pass_EE"))
                h_sta_fail.Add(load(f"h_mass_Sta_fail_BE"))
                h_sta_fail.Add(load(f"h_mass_Sta_fail_EE"))

            ROOT.calculateDataEfficiency(h_sta_pass, h_sta_fail, m, "Sta", etaRegion, sigModel, bkgModel, sigModel, bkgModel, hPV, sigTemplates)

        elif mode == "DQMData":

            # load histograms with probes that fail global muon requirement, for global muon efficiency
            h_glo_fail = load(f"h_mass_Glo_fail_BB")

            if etaRegion == "I":
                h_glo_fail.Add(load(f"h_mass_Glo_fail_BE"))
                h_glo_fail.Add(load(f"h_mass_Glo_fail_EE"))

            ROOT.getZyield(h_glo_fail, m, "Glo", etaRegion, sigModel, bkgModel, 0, sigTemplates, hPV)

        exit()
        
        log.info("Load fit results ...")
        
        def load_from_workspace(type_eff):
            if type_eff == "yield":
                if etaRegion == "I":
                    filename_eff = f"{dirOutSub}/workspace_I_{m}.root"     # To run for the etaRegion=I
                elif etaRegion == "B":
                    filename_eff = f"{dirOutSub}/workspace_BB_{m}.root"    # To run for the etaRegion=B
            else:
                filename_eff = f"{dirOutSub}/workspace_{type_eff}_{etaRegion}_{m}.root"
                
            if not os.path.isfile(filename_eff):
                log.warning(f"fit result {filename_eff} does not exist! exit")
                return None
                
            tfile_eff = ROOT.TFile(filename_eff,"READ")
            workspace = tfile_eff.Get("workspace")
            eff = workspace.var("eff").getVal()
                        
            if type_eff != "yield":
                return eff
            else:
                nsig = workspace.var("Nsig").getVal()
                return nsig, eff
                
        
        recoZ, effHLT = load_from_workspace("yield")
        effSel = load_from_workspace("Sel")
        effGlo = load_from_workspace("Glo")
        effSta = load_from_workspace("Sta")
        
        # trigger efficiency that at least one muon passes the trigger
        # effTrigger = (1 - (1-effHLT)**2 )
        # reconstructed number of Zs, do not perform fit but assume 1% background
        # recoZ = h_mass_yield_Z.Integral() * 0.99
        # delivered number of Zs given by efficiency corrected number of reconstructed Zs
        # delZ = recoZ / (effTrigger * effSel**2 * effGlo**2)

        # correlation factor between inner and outer track
        cIO = getCorrelationIO(hPV, correlationsIO)

        # calculate efficiency corrected number of Z bosons
        delZ = recoZ * cIO**2 / (effSel**2 * effGlo**2 * effSta**2)

        log.info(f"Delivered number of Zs = {delZ}")

        # deadtime during the good lumisections
        deadtime = byLS_m['recorded(/pb)'].sum() / byLS_m['delivered(/pb)'].sum()
        
        # amount of seconds where we measured the Z counts (during good lumisections)
        timewindow = len(byLS_m)*secPerLS

        # convert time string to datetime format
        beginTime = to_DateTime(byLS_m['time'][0], string_format = "mm/dd/yy")
        endTime = to_DateTime(byLS_m['time'][-1], string_format = "mm/dd/yy") + datetime.timedelta(seconds=secPerLS)
        
        # total time window from the beginning of the first to the end of the last lumisection
        totaltimewindow = (endTime - beginTime).total_seconds()
        
        result = {
            "fill": fill,
            "run": run,
            "measurement": m,
            "beginTime": beginTime.strftime("%y/%m/%d %H:%M:%S"),
            "endTime": endTime.strftime("%y/%m/%d %H:%M:%S") ,
            "deadtime": deadtime,            
            "timewindow": timewindow,
            "totaltimewindow": totaltimewindow,
            "delLumi": byLS_m['delivered(/pb)'].sum(),
            "recLumi": byLS_m['recorded(/pb)'].sum(),
            "instDelLumi": byLS_m['delivered(/pb)'].sum() / timewindow,
            "instRecLumi": byLS_m['recorded(/pb)'].sum() / timewindow,
            "pileUp": byLS_m['avgpu'].mean(),
            "effHLT": effHLT,
            "effSel": effSel,
            "effGlo": effGlo,
            "effSta": effSta,
            "cIO":cIO,
            # delivered Z rate (corrected for deadtime and extrapolatet to the total time window)
            "delZCount": delZ / deadtime * (totaltimewindow / timewindow),    
            # instantaneous delivered Z rate (corrected for deadtime)
            "ZRate": delZ / (timewindow * deadtime)   
            
        }
        results.append(result)

    # make one dataframe with information of the measurements
    df_results = pd.concat([pd.DataFrame([result]) for result in results], ignore_index=True, sort=False)

    if not os.path.isdir(dirOutCSV):
        os.mkdir(dirOutCSV)
        
    # write one single csv file for each run             
    with open(f'{dirOutCSV}/csvfile{run}.csv', 'w') as file:
        df_results.to_csv(file, index=False)

# write one large file containg full information of all measurements
writeSummaryCSV(dirOutCSV, outName=f"Mergedcsvfile_{etaCut}", writeByLS=False)

# write one large file containg CMS information of all measurements - to share with ATLAS
writeSummaryCSV(dirOutCSV, outName=f"cms_{etaCut}", writeByLS=False, 
    keys=["fill","beginTime","endTime","ZRate","instDelLumi","delLumi","delZCount"])

log.info(" ===Done")
